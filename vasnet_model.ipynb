{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1whKIcT60CT-HwQX-lHMJ2SaxJRsFCYvG",
      "authorship_tag": "ABX9TyPfStgPOz4p4n0sez/fpYUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyegupta12/videosummcolab/blob/main/vasnet_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ojHZlk6pHi8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from layer_norm import  *\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, apperture=-1, ignore_itself=False, input_size=1024, output_size=1024):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.apperture = apperture\n",
        "        self.ignore_itself = ignore_itself\n",
        "\n",
        "        self.m = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.K = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
        "        self.Q = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
        "        self.V = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
        "        self.output_linear = nn.Linear(in_features=self.output_size, out_features=self.m, bias=False)\n",
        "\n",
        "        self.drop50 = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.shape[0]  # sequence length\n",
        "\n",
        "        K = self.K(x)  # ENC (n x m) => (n x H) H= hidden size\n",
        "        Q = self.Q(x)  # ENC (n x m) => (n x H) H= hidden size\n",
        "        V = self.V(x)\n",
        "\n",
        "        Q *= 0.06\n",
        "        logits = torch.matmul(Q, K.transpose(1,0))\n",
        "\n",
        "        if self.ignore_itself:\n",
        "            # Zero the diagonal activations (a distance of each frame with itself)\n",
        "            logits[torch.eye(n).byte()] = -float(\"Inf\")\n",
        "\n",
        "        if self.apperture > 0:\n",
        "            # Set attention to zero to frames further than +/- apperture from the current one\n",
        "            onesmask = torch.ones(n, n)\n",
        "            trimask = torch.tril(onesmask, -self.apperture) + torch.triu(onesmask, self.apperture)\n",
        "            logits[trimask == 1] = -float(\"Inf\")\n",
        "\n",
        "        att_weights_ = nn.functional.softmax(logits, dim=-1)\n",
        "        weights = self.drop50(att_weights_)\n",
        "        y = torch.matmul(V.transpose(1,0), weights).transpose(1,0)\n",
        "        y = self.output_linear(y)\n",
        "\n",
        "        return y, att_weights_\n",
        "\n",
        "\n",
        "\n",
        "class VASNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VASNet, self).__init__()\n",
        "\n",
        "        self.m = 1024 # cnn features size\n",
        "        self.hidden_size = 1024\n",
        "\n",
        "        self.att = SelfAttention(input_size=self.m, output_size=self.m)\n",
        "        self.ka = nn.Linear(in_features=self.m, out_features=1024)\n",
        "        self.kb = nn.Linear(in_features=self.ka.out_features, out_features=1024)\n",
        "        self.kc = nn.Linear(in_features=self.kb.out_features, out_features=1024)\n",
        "        self.kd = nn.Linear(in_features=self.ka.out_features, out_features=1)\n",
        "\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop50 = nn.Dropout(0.5)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.layer_norm_y = LayerNorm(self.m)\n",
        "        self.layer_norm_ka = LayerNorm(self.ka.out_features)\n",
        "\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "\n",
        "        m = x.shape[2] # Feature size\n",
        "\n",
        "        # Place the video frames to the batch dimension to allow for batch arithm. operations.\n",
        "        # Assumes input batch size = 1.\n",
        "        x = x.view(-1, m)\n",
        "        y, att_weights_ = self.att(x)\n",
        "\n",
        "        y = y + x\n",
        "        y = self.drop50(y)\n",
        "        y = self.layer_norm_y(y)\n",
        "\n",
        "        # Frame level importance score regression\n",
        "        # Two layer NN\n",
        "        y = self.ka(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.drop50(y)\n",
        "        y = self.layer_norm_ka(y)\n",
        "\n",
        "        y = self.kd(y)\n",
        "        y = self.sig(y)\n",
        "        y = y.view(1, -1)\n",
        "\n",
        "        return y, att_weights_\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"done\")\n",
        "    pass"
      ]
    }
  ]
}